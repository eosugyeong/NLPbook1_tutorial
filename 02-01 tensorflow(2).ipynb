{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './data_out/checkpoint/dnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A81502E948>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./data_out/checkpoint/dnn\\model.ckpt-1200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into ./data_out/checkpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.5503301e-05, step = 1201\n",
      "INFO:tensorflow:global_step/sec: 174.987\n",
      "INFO:tensorflow:loss = 2.1478587e-05, step = 1301 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.445\n",
      "INFO:tensorflow:loss = 6.6957587e-06, step = 1401 (0.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 468.463\n",
      "INFO:tensorflow:loss = 5.7862603e-06, step = 1501 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 396.365\n",
      "INFO:tensorflow:loss = 5.076528e-06, step = 1601 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 327.67\n",
      "INFO:tensorflow:loss = 1.3287246e-05, step = 1701 (0.305 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into ./data_out/checkpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.5900663e-06.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x2a81ef37ec8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "samples = ['너 오늘 이뻐 보인다',\n",
    "          '나는 오늘 기분이 더러워',\n",
    "          '끝내주는데, 좋은 일이 있나봐',\n",
    "          '나 좋은 일이 생겼어',\n",
    "          '아 오늘 진짜 짜증나',\n",
    "          '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "label = [[1], [0], [1], [1], [0], [1]]\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "    dataset = dataset.repeat(EPOCH)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.shuffle(len(sequences))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "VOCAB_SIZE = len(word_index) + 1\n",
    "EMB_SIZE = 128\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    embed_input = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features)\n",
    "    embed_input = tf.reduce_mean(embed_input, axis = 1)\n",
    "    \n",
    "    hidden_layer = tf.keras.layers.Dense(128, activation = tf.nn.relu)(embed_input)\n",
    "    output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "    output = tf.nn.sigmoid(output_layer)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(output, labels)\n",
    "\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        train_op = train_op,\n",
    "        loss = loss)\n",
    "\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn = model_fn,  model_dir = DATA_OUT_PATH + 'checkpoint/dnn')\n",
    "estimator.train(train_input_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
