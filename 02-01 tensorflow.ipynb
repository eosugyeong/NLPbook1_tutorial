{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(input)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "    filters = 10,\n",
    "    kernel_size = 3,\n",
    "    padding = 'same',\n",
    "    activation = tf.nn.relu)(dropout)\n",
    "max_pool = tf.keras.layers.MaxPool1D(\n",
    "    pool_size = 3,\n",
    "    padding = 'same')(conv)\n",
    "flatten = tf.keras.layers.Flatten()(max_pool)\n",
    "hidden = tf.keras.layers.Dense(units = 50, activation = tf.nn.relu)(flatten)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.softmax)(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['너 오늘 이뻐 보인다',\n",
    "          '나는 오늘 기분이 더러워',\n",
    "          '끝내주는데, 좋은 일이 있나봐',\n",
    "          '나 좋은 일이 생겼어',\n",
    "          '아 오늘 진짜 짜증나',\n",
    "          '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "label = [[1], [0], [1], [1], [0], [1]]\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4, 1, 5, 6]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[4, 1, 5, 6],\n",
      "       [7, 1, 8, 9]]), array([[1],\n",
      "       [0]]))\n",
      "(array([[14,  1, 15, 16],\n",
      "       [17, 18, 19, 20]]), array([[0],\n",
      "       [1]]))\n",
      "(array([[14,  1, 15, 16],\n",
      "       [17, 18, 19, 20]]), array([[0],\n",
      "       [1]]))\n",
      "(array([[10,  2,  3, 11],\n",
      "       [12,  2,  3, 13]]), array([[1],\n",
      "       [1]]))\n",
      "(array([[10,  2,  3, 11],\n",
      "       [12,  2,  3, 13]]), array([[1],\n",
      "       [1]]))\n",
      "(array([[4, 1, 5, 6],\n",
      "       [7, 1, 8, 9]]), array([[1],\n",
      "       [0]]))\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "EPOCH = 2\n",
    "dataset = dataset.repeat(EPOCH)\n",
    "BATCH_SIZE = 2\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.shuffle(len(sequences))\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'x': array([[10,  2,  3, 11],\n",
      "       [12,  2,  3, 13]])}, array([[1],\n",
      "       [1]]))\n",
      "({'x': array([[4, 1, 5, 6],\n",
      "       [7, 1, 8, 9]])}, array([[1],\n",
      "       [0]]))\n",
      "({'x': array([[14,  1, 15, 16],\n",
      "       [17, 18, 19, 20]])}, array([[0],\n",
      "       [1]]))\n",
      "({'x': array([[4, 1, 5, 6],\n",
      "       [7, 1, 8, 9]])}, array([[1],\n",
      "       [0]]))\n",
      "({'x': array([[14,  1, 15, 16],\n",
      "       [17, 18, 19, 20]])}, array([[0],\n",
      "       [1]]))\n",
      "({'x': array([[10,  2,  3, 11],\n",
      "       [12,  2,  3, 13]])}, array([[1],\n",
      "       [1]]))\n"
     ]
    }
   ],
   "source": [
    "def mapping_fn(X, Y=None):\n",
    "    input = {'x': X}\n",
    "    label = Y\n",
    "    return input, label\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCH = 2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataset = dataset.map(mapping_fn)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.shuffle(len(sequences))\n",
    "dataset = dataset.repeat(EPOCH)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/ehrme/data_out/checkpoint/dnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002B340E38DC8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From C:\\Users\\ehrme\\Anaconda3\\envs\\sugyeong_2019\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /Users/ehrme/data_out/checkpoint/dnn\\model.ckpt-600\n",
      "WARNING:tensorflow:From C:\\Users\\ehrme\\Anaconda3\\envs\\sugyeong_2019\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 600 into /Users/ehrme/data_out/checkpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:loss = 3.2957716e-05, step = 601\n",
      "INFO:tensorflow:global_step/sec: 169.085\n",
      "INFO:tensorflow:loss = 2.4329782e-05, step = 701 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.685\n",
      "INFO:tensorflow:loss = 2.071154e-05, step = 801 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.418\n",
      "INFO:tensorflow:loss = 2.8746932e-05, step = 901 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 399.472\n",
      "INFO:tensorflow:loss = 1.207984e-05, step = 1001 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.075\n",
      "INFO:tensorflow:loss = 3.1565167e-05, step = 1101 (0.358 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into /Users/ehrme/data_out/checkpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.986851e-05.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x2b340e38848>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "samples = ['너 오늘 이뻐 보인다',\n",
    "          '나는 오늘 기분이 더러워',\n",
    "          '끝내주는데, 좋은 일이 있나봐',\n",
    "          '나 좋은 일이 생겼어',\n",
    "          '아 오늘 진짜 짜증나',\n",
    "          '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "label = [[1], [0], [1], [1], [0], [1]]\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "EPOCH = 100\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "    dataset = dataset.repeat(EPOCH)\n",
    "    dataset = dataset.batch(1)\n",
    "    dataset = dataset.shuffle(len(sequences))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "VOCAB_SIZE = len(word_index) + 1\n",
    "EMB_SIZE = 128\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    embed_input = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features)\n",
    "    embed_input = tf.reduce_mean(embed_input, axis = 1)\n",
    "    \n",
    "    hidden_layer = tf.keras.layers.Dense(128, activation = tf.nn.relu)(embed_input)\n",
    "    output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "    output = tf.nn.sigmoid(output_layer)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(output, labels)\n",
    "    \n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode = mode,\n",
    "            train_op = train_op,\n",
    "            loss = loss)\n",
    "    \n",
    "DATA_OUT_PATH = '/Users/ehrme/data_out/'\n",
    "    \n",
    "import os\n",
    "    \n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "        \n",
    "estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir = DATA_OUT_PATH + 'checkpoint/dnn')\n",
    "estimator.train(train_input_fn)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
